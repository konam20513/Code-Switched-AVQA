{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-925d1f5ae4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "video_directory = \"/home/ankitk/Desktop/AVQA/MUSIC-AVQA-videos-Real-002/MUSIC-AVQA-videos-Real\"  # Replace with the actual directory path\n",
    "video_file=[]\n",
    "for index, row in train_df.iterrows():\n",
    "    video_id = row['video_id']\n",
    "    video_file_path = os.path.join(video_directory, f\"{video_id}.mp4\")\n",
    "    video_file.append(video_file_path)\n",
    "#     print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"video_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VideoEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def extract_video_features(file_path, device):\n",
    "    container = av.open(file_path)\n",
    "\n",
    "    indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    features = last_hidden_states.mean(dim=1).squeeze().to(\"cpu\").numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "video_files = video_file\n",
    "all_features = []\n",
    "count = 0\n",
    "for file_path in video_files:\n",
    "    features = extract_video_features(file_path, device)\n",
    "    print(f\"Features for {file_path}:\")\n",
    "    print(features.shape)  \n",
    "    all_features.append(features)\n",
    "    \n",
    "all_features_array = np.array(all_features)\n",
    "\n",
    "np.save(\"./video_features.npy\", all_features_array)\n",
    "\n",
    "print(\"Processing completed for all videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEModel, ASTFeatureExtractor, ASTModel\n",
    "\n",
    "\n",
    "audio_folder = \"/home/ankitk/Desktop/AVQA/Audio_wav\"\n",
    "\n",
    "embeddings_folder = \"/home/ankitk/Desktop/AVQA/Audio_features\"\n",
    "os.makedirs(embeddings_folder, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "audio_processor = ASTFeatureExtractor()\n",
    "audio_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\").to(device)\n",
    "\n",
    "\n",
    "for filename in os.listdir(audio_folder):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_segment_path = os.path.join(audio_folder, filename)\n",
    "\n",
    "        stereo_audio, sample_rate = torchaudio.load(audio_segment_path)\n",
    "\n",
    "        mono_audio = stereo_audio.mean(dim=0)\n",
    "\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        mono_audio_16khz = resample(mono_audio)\n",
    "\n",
    "        inputs = audio_processor(mono_audio_16khz, sampling_rate=16000, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = audio_model(**inputs)\n",
    "\n",
    "        last_hidden_states = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "        embedding_path = os.path.join(embeddings_folder, f\"{filename[:-4]}.npy\")\n",
    "        np.save(embedding_path, last_hidden_states.cpu().numpy())\n",
    "\n",
    "        print(f\"Audio embedding saved for {filename}\")\n",
    "\n",
    "print(\"All audio embeddings saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
